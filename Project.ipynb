{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from  tensorflow.contrib.learn.python.learn.estimators.dnn  import DNNClassifier\n",
    "from tensorflow.contrib.layers import real_valued_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apple = pd.read_csv('merged_df.csv')\n",
    "#googl = pd.read_csv('WIKI_PRICES_GOOGL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Count</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ex-dividend</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>adj_open</th>\n",
       "      <th>adj_high</th>\n",
       "      <th>adj_low</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>adj_volume</th>\n",
       "      <th>diff</th>\n",
       "      <th>bin_sentiment</th>\n",
       "      <th>bin_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>53</td>\n",
       "      <td>133</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>590.14</td>\n",
       "      <td>601.0000</td>\n",
       "      <td>590.00</td>\n",
       "      <td>600.9600</td>\n",
       "      <td>10252400.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.229879</td>\n",
       "      <td>80.687900</td>\n",
       "      <td>79.211083</td>\n",
       "      <td>80.682530</td>\n",
       "      <td>71766800.0</td>\n",
       "      <td>10.8200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-05-06</td>\n",
       "      <td>15</td>\n",
       "      <td>177</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>601.80</td>\n",
       "      <td>604.4099</td>\n",
       "      <td>594.41</td>\n",
       "      <td>594.4100</td>\n",
       "      <td>13377300.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.795305</td>\n",
       "      <td>81.145700</td>\n",
       "      <td>79.803152</td>\n",
       "      <td>79.803152</td>\n",
       "      <td>93641100.0</td>\n",
       "      <td>-7.3900</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>40</td>\n",
       "      <td>134</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>595.25</td>\n",
       "      <td>597.2900</td>\n",
       "      <td>587.73</td>\n",
       "      <td>592.3300</td>\n",
       "      <td>10102300.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.915927</td>\n",
       "      <td>80.189810</td>\n",
       "      <td>78.906322</td>\n",
       "      <td>79.523900</td>\n",
       "      <td>70716100.0</td>\n",
       "      <td>-2.9200</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>31</td>\n",
       "      <td>109</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>588.25</td>\n",
       "      <td>594.4100</td>\n",
       "      <td>586.40</td>\n",
       "      <td>587.9900</td>\n",
       "      <td>8224900.0</td>\n",
       "      <td>3.29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.418033</td>\n",
       "      <td>80.249678</td>\n",
       "      <td>79.168269</td>\n",
       "      <td>79.382931</td>\n",
       "      <td>57574300.0</td>\n",
       "      <td>-0.2600</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2014-05-09</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>584.54</td>\n",
       "      <td>586.2500</td>\n",
       "      <td>580.33</td>\n",
       "      <td>585.5425</td>\n",
       "      <td>10414200.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.917156</td>\n",
       "      <td>79.148018</td>\n",
       "      <td>78.348775</td>\n",
       "      <td>79.052500</td>\n",
       "      <td>72899400.0</td>\n",
       "      <td>1.0025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date  sentiment  Count ticker    open      high     low  \\\n",
       "0           0  2014-05-05         53    133   AAPL  590.14  601.0000  590.00   \n",
       "1           1  2014-05-06         15    177   AAPL  601.80  604.4099  594.41   \n",
       "2           2  2014-05-07         40    134   AAPL  595.25  597.2900  587.73   \n",
       "3           3  2014-05-08         31    109   AAPL  588.25  594.4100  586.40   \n",
       "4           4  2014-05-09          3     67   AAPL  584.54  586.2500  580.33   \n",
       "\n",
       "      close      volume  ex-dividend  split_ratio   adj_open   adj_high  \\\n",
       "0  600.9600  10252400.0         0.00          1.0  79.229879  80.687900   \n",
       "1  594.4100  13377300.0         0.00          1.0  80.795305  81.145700   \n",
       "2  592.3300  10102300.0         0.00          1.0  79.915927  80.189810   \n",
       "3  587.9900   8224900.0         3.29          1.0  79.418033  80.249678   \n",
       "4  585.5425  10414200.0         0.00          1.0  78.917156  79.148018   \n",
       "\n",
       "     adj_low  adj_close  adj_volume     diff  bin_sentiment  bin_diff  \n",
       "0  79.211083  80.682530  71766800.0  10.8200              1         1  \n",
       "1  79.803152  79.803152  93641100.0  -7.3900              1        -1  \n",
       "2  78.906322  79.523900  70716100.0  -2.9200              1        -1  \n",
       "3  79.168269  79.382931  57574300.0  -0.2600              1        -1  \n",
       "4  78.348775  79.052500  72899400.0   1.0025              1         1  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>adj_open</th>\n",
       "      <th>adj_high</th>\n",
       "      <th>adj_low</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>adj_volume</th>\n",
       "      <th>bin_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-05</td>\n",
       "      <td>79.229879</td>\n",
       "      <td>80.687900</td>\n",
       "      <td>79.211083</td>\n",
       "      <td>80.682530</td>\n",
       "      <td>71766800.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-05-06</td>\n",
       "      <td>80.795305</td>\n",
       "      <td>81.145700</td>\n",
       "      <td>79.803152</td>\n",
       "      <td>79.803152</td>\n",
       "      <td>93641100.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>79.915927</td>\n",
       "      <td>80.189810</td>\n",
       "      <td>78.906322</td>\n",
       "      <td>79.523900</td>\n",
       "      <td>70716100.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-08</td>\n",
       "      <td>79.418033</td>\n",
       "      <td>80.249678</td>\n",
       "      <td>79.168269</td>\n",
       "      <td>79.382931</td>\n",
       "      <td>57574300.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-09</td>\n",
       "      <td>78.917156</td>\n",
       "      <td>79.148018</td>\n",
       "      <td>78.348775</td>\n",
       "      <td>79.052500</td>\n",
       "      <td>72899400.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   adj_open   adj_high    adj_low  adj_close  adj_volume  \\\n",
       "0  2014-05-05  79.229879  80.687900  79.211083  80.682530  71766800.0   \n",
       "1  2014-05-06  80.795305  81.145700  79.803152  79.803152  93641100.0   \n",
       "2  2014-05-07  79.915927  80.189810  78.906322  79.523900  70716100.0   \n",
       "3  2014-05-08  79.418033  80.249678  79.168269  79.382931  57574300.0   \n",
       "4  2014-05-09  78.917156  79.148018  78.348775  79.052500  72899400.0   \n",
       "\n",
       "   bin_diff  \n",
       "0         1  \n",
       "1        -1  \n",
       "2        -1  \n",
       "3        -1  \n",
       "4         1  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl = apple.copy().drop(apple.columns[[0,2,3, 4, 5, 6, 7, 8, 9, 10, 11, 17, 18, ]], axis = 1)\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(C=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InputDF = aapl.copy().drop('bin_diff', axis = 1)\n",
    "InputDF = InputDF.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = pd.qcut(apple['diff'], 2,labels=range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 1, ..., 0, 0, 1, 0, 1]\n",
       "Length: 779\n",
       "Categories (2, int64): [0 < 1]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(779, 5)\n"
     ]
    }
   ],
   "source": [
    "InputDF.head()\n",
    "print (InputDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "InputDF = InputDF.apply(lambda x:(x -x.mean())/x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj_open</th>\n",
       "      <th>adj_high</th>\n",
       "      <th>adj_low</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>adj_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-05-05</th>\n",
       "      <td>-1.923325</td>\n",
       "      <td>-1.890850</td>\n",
       "      <td>-1.866902</td>\n",
       "      <td>-1.833370</td>\n",
       "      <td>1.255323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-06</th>\n",
       "      <td>-1.825542</td>\n",
       "      <td>-1.862243</td>\n",
       "      <td>-1.829929</td>\n",
       "      <td>-1.888248</td>\n",
       "      <td>2.270155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-07</th>\n",
       "      <td>-1.880472</td>\n",
       "      <td>-1.921975</td>\n",
       "      <td>-1.885933</td>\n",
       "      <td>-1.905676</td>\n",
       "      <td>1.206577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-08</th>\n",
       "      <td>-1.911572</td>\n",
       "      <td>-1.918234</td>\n",
       "      <td>-1.869576</td>\n",
       "      <td>-1.914473</td>\n",
       "      <td>0.596879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-09</th>\n",
       "      <td>-1.942859</td>\n",
       "      <td>-1.987076</td>\n",
       "      <td>-1.920750</td>\n",
       "      <td>-1.935094</td>\n",
       "      <td>1.307868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            adj_open  adj_high   adj_low  adj_close  adj_volume\n",
       "date                                                           \n",
       "2014-05-05 -1.923325 -1.890850 -1.866902  -1.833370    1.255323\n",
       "2014-05-06 -1.825542 -1.862243 -1.829929  -1.888248    2.270155\n",
       "2014-05-07 -1.880472 -1.921975 -1.885933  -1.905676    1.206577\n",
       "2014-05-08 -1.911572 -1.918234 -1.869576  -1.914473    0.596879\n",
       "2014-05-09 -1.942859 -1.987076 -1.920750  -1.935094    1.307868"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = 600\n",
    "xtrain, xtest = InputDF.iloc[:test_size, :], InputDF.iloc[test_size:, :]\n",
    "ytrain, ytest = label[:test_size], label[test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 5)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = (InputDF[:-test_size].values,label[:-test_size].values)\n",
    "v = (InputDF[-test_size:].values,label[-test_size:].values)\n",
    "\n",
    "t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = logreg.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99        83\n",
      "          1       0.99      1.00      0.99        96\n",
      "\n",
      "avg / total       0.99      0.99      0.99       179\n",
      "\n",
      "[[82  1]\n",
      " [ 0 96]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print (classification_report(ytest , res.predict(xtest)))\n",
    "print (confusion_matrix(ytest, res.predict(xtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training basic feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(InputDF.columns)\n",
    "dropout=0.2\n",
    "hidden_1_size = 100\n",
    "hidden_2_size = 25\n",
    "num_classes = label.nunique()\n",
    "NUM_EPOCHS=10\n",
    "BATCH_SIZE=50\n",
    "lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = (xtrain, ytrain.values)\n",
    "# val = (xtest, ytest.values)\n",
    "val = (InputDF[:-test_size].values, label[:-test_size].values)\n",
    "train = (InputDF[-test_size:].values, label[-test_size:].values)\n",
    "NUM_TRAIN_BATCHES = int(len(train[0])/BATCH_SIZE)\n",
    "NUM_VAL_BATCHES = int(len(val[1])/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (NUM_TRAIN_BATCHES)\n",
    "int(len(xtrain)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (NUM_VAL_BATCHES)\n",
    "int(len(xtest)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        self.input_data = tf.placeholder(dtype=tf.float32,shape=[None,num_features])\n",
    "        self.target_data = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "        self.dropout_prob = tf.placeholder(dtype=tf.float32,shape=[])\n",
    "        with tf.variable_scope(\"ff\"):\n",
    "            droped_input = tf.nn.dropout(self.input_data,keep_prob=self.dropout_prob)\n",
    "            \n",
    "            layer_1 = tf.contrib.layers.fully_connected(\n",
    "                num_outputs=hidden_1_size,\n",
    "                inputs=droped_input,\n",
    "            )\n",
    "            layer_2 = tf.contrib.layers.fully_connected(\n",
    "                num_outputs=hidden_2_size,\n",
    "                inputs=layer_1,\n",
    "            )\n",
    "            self.logits = tf.contrib.layers.fully_connected(\n",
    "                num_outputs=num_classes,\n",
    "                activation_fn =None,\n",
    "                inputs=layer_2,\n",
    "            )\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            \n",
    "            self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits, \n",
    "                                                                         labels = self.target_data)\n",
    "            mask = (1-tf.sign(1-self.target_data)) #Don't give credit for flat days\n",
    "            mask = tf.cast(mask,tf.float32)\n",
    "            self.loss = tf.reduce_sum(self.losses)\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            opt = tf.train.AdamOptimizer(lr)\n",
    "            gvs = opt.compute_gradients(self.loss)\n",
    "            self.train_op = opt.apply_gradients(gvs, global_step=global_step)\n",
    "        \n",
    "        with tf.name_scope(\"predictions\"):\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "            self.predictions = tf.argmax(self.probs, 1)\n",
    "            correct_pred = tf.cast(tf.equal(self.predictions, tf.cast(self.target_data,tf.int64)),tf.float64)\n",
    "            self.accuracy = tf.reduce_mean(correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-97-51114dc70183>:6: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step - 12 loss - 418.0542907714844 acc - 0.58\n",
      "step - 24 loss - 417.95225524902344 acc - 0.6\n",
      "step - 36 loss - 416.98297119140625 acc - 0.6\n",
      "step - 48 loss - 416.33203125 acc - 0.6\n",
      "step - 60 loss - 414.8281707763672 acc - 0.64\n",
      "step - 72 loss - 415.0836410522461 acc - 0.6\n",
      "step - 84 loss - 414.72016525268555 acc - 0.6\n",
      "step - 96 loss - 415.8470573425293 acc - 0.6\n",
      "step - 108 loss - 414.93212127685547 acc - 0.6\n",
      "step - 120 loss - 413.87714767456055 acc - 0.6\n",
      "done training\n",
      "0.48\n",
      "0.52\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    model = Model()\n",
    "    input_ = train[0]\n",
    "    target = train[1]\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess.run([init])\n",
    "        epoch_loss =0\n",
    "        for e in range(NUM_EPOCHS):\n",
    "            if epoch_loss >0 and epoch_loss <1:\n",
    "                break\n",
    "            epoch_loss =0\n",
    "            for batch in range(0,NUM_TRAIN_BATCHES):\n",
    "                \n",
    "                start = batch*BATCH_SIZE\n",
    "                end = start + BATCH_SIZE \n",
    "                feed = {\n",
    "                    model.input_data:input_[start:end],\n",
    "                    model.target_data:target[start:end],\n",
    "                    model.dropout_prob:0.9\n",
    "                            }\n",
    "                \n",
    "                _,loss,acc = sess.run(\n",
    "                    [\n",
    "                        model.train_op,\n",
    "                        model.loss,\n",
    "                        model.accuracy,\n",
    "                    ]\n",
    "                    ,feed_dict=feed\n",
    "                )\n",
    "                epoch_loss+=loss\n",
    "            print('step - {0} loss - {1} acc - {2}'.format((1+batch+NUM_TRAIN_BATCHES*e),epoch_loss,acc))\n",
    "                \n",
    "        \n",
    "        print('done training')\n",
    "        final_preds =np.array([])\n",
    "        final_probs =None\n",
    "        for batch in range(0,NUM_VAL_BATCHES):\n",
    "            \n",
    "                start = batch*BATCH_SIZE\n",
    "                end = start + BATCH_SIZE \n",
    "                feed = {\n",
    "                    model.input_data:val[0][start:end],\n",
    "                    model.target_data:val[1][start:end],\n",
    "                    model.dropout_prob:1\n",
    "                            }\n",
    "                \n",
    "                acc,preds,probs = sess.run(\n",
    "                    [\n",
    "                        model.accuracy,\n",
    "                        model.predictions,\n",
    "                        model.probs\n",
    "                    ]\n",
    "                    ,feed_dict=feed\n",
    "                )\n",
    "                print(acc)\n",
    "                final_preds = np.concatenate((final_preds,preds),axis=0)\n",
    "                if final_probs is None:\n",
    "                    final_probs = probs\n",
    "                else:\n",
    "                    final_probs = np.concatenate((final_probs,probs),axis=0)\n",
    "        prediction_conf = final_probs[np.argmax(final_probs, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [179, 150]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-fb7e6f18f642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#ytest.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                                   \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                                                   \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m                                                   sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [179, 150]"
     ]
    }
   ],
   "source": [
    "print (classification_report(label[:-test_size].values, final_preds))\n",
    "#ytest.shape\n",
    "print (final_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers.python.layers.initializers import xavier_initializer\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "RNN_HIDDEN_SIZE=100\n",
    "FIRST_LAYER_SIZE=1000\n",
    "SECOND_LAYER_SIZE=250\n",
    "NUM_LAYERS=2\n",
    "BATCH_SIZE=50\n",
    "NUM_EPOCHS=10\n",
    "lr=0.0003\n",
    "NUM_TRAIN_BATCHES = int(len(train[0])/BATCH_SIZE)\n",
    "NUM_VAL_BATCHES = int(len(val[1])/BATCH_SIZE)\n",
    "ATTN_LENGTH=30\n",
    "beta=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNModel():\n",
    "    def __init__(self):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        self.input_data = tf.placeholder(dtype=tf.float32,shape=[BATCH_SIZE,num_features])\n",
    "        self.target_data = tf.placeholder(dtype=tf.int32,shape=[BATCH_SIZE])\n",
    "        self.dropout_prob = tf.placeholder(dtype=tf.float32,shape=[])\n",
    "        \n",
    "        def makeGRUCells():\n",
    "            base_cell = rnn.GRUCell(num_units=RNN_HIDDEN_SIZE,) \n",
    "            layered_cell = rnn.MultiRNNCell([base_cell] * NUM_LAYERS,state_is_tuple=False) \n",
    "            attn_cell =tf.contrib.rnn.AttentionCellWrapper(cell=layered_cell,attn_length=ATTN_LENGTH,state_is_tuple=False)\n",
    "            return attn_cell\n",
    "        \n",
    "        self.gru_cell = makeGRUCells()\n",
    "        self.zero_state = self.gru_cell.zero_state(1, tf.float32)\n",
    "        \n",
    "        self.start_state = tf.placeholder(dtype=tf.float32,shape=[1,self.gru_cell.state_size])\n",
    "        \n",
    "        \n",
    "\n",
    "        with tf.variable_scope(\"ff\",initializer=xavier_initializer(uniform=False)):\n",
    "            droped_input = tf.nn.dropout(self.input_data,keep_prob=self.dropout_prob)\n",
    "            \n",
    "            layer_1 = tf.contrib.layers.fully_connected(\n",
    "                num_outputs=FIRST_LAYER_SIZE,\n",
    "                inputs=droped_input,\n",
    "                \n",
    "            )\n",
    "            layer_2 = tf.contrib.layers.fully_connected(\n",
    "                num_outputs=RNN_HIDDEN_SIZE,\n",
    "                inputs=layer_1,\n",
    "                \n",
    "            )\n",
    "            \n",
    "        \n",
    "        split_inputs = tf.reshape(droped_input,shape=[1,BATCH_SIZE,num_features],name=\"reshape_l1\") # Each item in the batch is a time step, iterate through them\n",
    "        split_inputs = tf.unstack(split_inputs,axis=1,name=\"unpack_l1\")\n",
    "        states =[]\n",
    "        outputs =[]\n",
    "        with tf.variable_scope(\"rnn\",initializer=xavier_initializer(uniform=False)) as scope:\n",
    "            state = self.start_state\n",
    "            for i, inp in enumerate(split_inputs):\n",
    "                if i >0:\n",
    "                    scope.reuse_variables()\n",
    "                \n",
    "                output, state = self.gru_cell(inp, state)\n",
    "                states.append(state)\n",
    "                outputs.append(output)\n",
    "        self.end_state = states[-1]\n",
    "        outputs = tf.stack(outputs,axis=1) # Pack them back into a single tensor\n",
    "        outputs = tf.reshape(outputs,shape=[BATCH_SIZE,RNN_HIDDEN_SIZE])\n",
    "        self.logits = tf.contrib.layers.fully_connected(\n",
    "            num_outputs=num_classes,\n",
    "            inputs=outputs,\n",
    "            activation_fn=None\n",
    "        )\n",
    "\n",
    "            \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            self.penalties =    tf.reduce_sum([beta*tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "\n",
    "            \n",
    "            self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits,\n",
    "                                                                         labels = self.target_data)\n",
    "            self.loss = tf.reduce_sum(self.losses + beta*self.penalties)\n",
    "        \n",
    "        with tf.name_scope(\"train_step\"):\n",
    "            opt = tf.train.AdamOptimizer(lr)\n",
    "            gvs = opt.compute_gradients(self.loss)\n",
    "            self.train_op = opt.apply_gradients(gvs, global_step=global_step)\n",
    "        \n",
    "        with tf.name_scope(\"predictions\"):\n",
    "            probs = tf.nn.softmax(self.logits)\n",
    "            self.predictions = tf.argmax(probs, 1)\n",
    "            correct_pred = tf.cast(tf.equal(self.predictions, tf.cast(self.target_data,tf.int64)),tf.float64)\n",
    "            self.accuracy = tf.reduce_mean(correct_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x130c85ef0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "step - 0 loss - 423.2655487060547 acc - 0.54\n",
      "step - 1 loss - 424.90478515625 acc - 0.54\n",
      "step - 2 loss - 410.9787902832031 acc - 0.58\n",
      "step - 3 loss - 411.7039451599121 acc - 0.54\n",
      "step - 4 loss - 410.9993782043457 acc - 0.58\n",
      "step - 5 loss - 409.0864067077637 acc - 0.54\n",
      "step - 6 loss - 408.6769714355469 acc - 0.56\n",
      "step - 7 loss - 408.57334899902344 acc - 0.54\n",
      "step - 8 loss - 409.7863540649414 acc - 0.54\n",
      "step - 9 loss - 411.15460205078125 acc - 0.54\n",
      "0.54\n",
      "0.4\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    model = RNNModel()\n",
    "    input_ = train[0]\n",
    "    target = train[1]\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run([init])\n",
    "        loss = 2000\n",
    "        \n",
    "        for e in range(NUM_EPOCHS):\n",
    "            state = sess.run(model.zero_state)\n",
    "            epoch_loss =0\n",
    "            for batch in range(0,NUM_TRAIN_BATCHES):\n",
    "                start = batch*BATCH_SIZE\n",
    "                end = start + BATCH_SIZE \n",
    "                feed = {\n",
    "                    model.input_data:input_[start:end],\n",
    "                    model.target_data:target[start:end],\n",
    "                    model.dropout_prob:0.5,\n",
    "                    model.start_state:state\n",
    "                            }\n",
    "                _,loss,acc,state = sess.run(\n",
    "                    [\n",
    "                        model.train_op,\n",
    "                        model.loss,\n",
    "                        model.accuracy,\n",
    "                        model.end_state\n",
    "                    ]\n",
    "                    ,feed_dict=feed\n",
    "                )\n",
    "                epoch_loss+=loss\n",
    "                \n",
    "            print('step - {0} loss - {1} acc - {2}'.format((e),epoch_loss,acc))\n",
    "        final_preds =np.array([])\n",
    "        for batch in range(0,NUM_VAL_BATCHES):\n",
    "                start = batch*BATCH_SIZE\n",
    "                end = start + BATCH_SIZE \n",
    "                feed = {\n",
    "                    model.input_data:val[0][start:end],\n",
    "                    model.target_data:val[1][start:end],\n",
    "                    model.dropout_prob:1,\n",
    "                    model.start_state:state\n",
    "                            }\n",
    "                acc,preds,state = sess.run(\n",
    "                    [\n",
    "                        model.accuracy,\n",
    "                        model.predictions,\n",
    "                        model.end_state\n",
    "                    ]\n",
    "                    ,feed_dict=feed\n",
    "                )\n",
    "                print(acc)\n",
    "                assert len(preds) == BATCH_SIZE\n",
    "                final_preds = np.concatenate((final_preds,preds),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [179, 150]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-f4fe012d99d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/admin/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                                   \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                                                   \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m                                                   sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [179, 150]"
     ]
    }
   ],
   "source": [
    "print (classification_report(ytest, final_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
